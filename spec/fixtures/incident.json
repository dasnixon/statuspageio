{
  "id": "2220srj7t3zp",
  "components": [
    {
      "id": "string",
      "page_id": "string",
      "group_id": "string",
      "created_at": "2021-07-02T16:35:05Z",
      "updated_at": "2021-07-02T16:35:05Z",
      "group": true,
      "name": "string",
      "description": "string",
      "position": 0,
      "status": "operational",
      "showcase": true,
      "only_show_if_degraded": true,
      "automation_email": "string",
      "start_date": "2021-07-02"
    }
  ],
  "created_at": "2021-07-02T16:35:05Z",
  "impact": "critical",
  "impact_override": "minor",
  "incident_updates": [
    {
      "id": "string",
      "incident_id": "string",
      "affected_components": [
        {
          "code": "string",
          "name": "string",
          "old_status": "operational",
          "new_status": "operational"
        }
      ],
      "body": "string",
      "created_at": "2021-07-02T16:35:05Z",
      "custom_tweet": "string",
      "deliver_notifications": true,
      "display_at": "2021-07-02T16:35:05Z",
      "status": "investigating",
      "tweet_id": "string",
      "twitter_updated_at": "2021-07-02T16:35:05Z",
      "updated_at": "2021-07-02T16:35:05Z",
      "wants_twitter_update": true
    }
  ],
  "metadata": {
    "jira": {
      "issue_id": "value"
    }
  },
  "monitoring_at": "2021-07-02T16:35:05Z",
  "name": "Data Layer Migration",
  "page_id": "string",
  "postmortem_body": "##### Issue\nAt approximately 17:02 UTC on 2013-04-21, our master database server unexpectedly went unresponsive to all network.\nA reboot of the machine at 17:05 UTC resulted in a failed mount of a corrupted EBS volume, and we made the decision\nat that time to fail over the slave database.\n\n##### Resolution\nAt 17:12 UTC, the slave database had been successfully promoted to master and the application recovered enough to\naccept web traffic again. A new slave database node was created and placed into the rotation to guard against future\nmaster failures. The promoted slave database performed slowly for the next couple of hours as the query cache began\nto warm up, and eventually settled into a reasonable performance profile around 20:00 UTC.\n\n##### Future Mitigation Plans\nOver the past few months, we have been working on an overhaul to our data storage layer with a migration from a Postgres\nsetup to a distributed, fault-tolerant, multi-region data layer using Riak. This initiative has been prioritized, and\nthe migration will be performed in the coming weeks. We will notify our clients of the scheduled downtime via an\nincident on this status site, and via a blog post.\n",
  "postmortem_body_last_updated_at": "2021-07-02T16:35:05Z",
  "postmortem_ignored": true,
  "postmortem_notified_subscribers": true,
  "postmortem_notified_twitter": true,
  "postmortem_published_at": true,
  "resolved_at": "2021-07-02T16:35:05Z",
  "scheduled_auto_completed": true,
  "scheduled_auto_in_progress": true,
  "scheduled_for": "2013-05-07T03:00:00.007Z",
  "scheduled_remind_prior": true,
  "scheduled_reminded_at": "2021-07-02T16:35:05Z",
  "scheduled_until": "2013-05-07T06:00:00.007Z",
  "shortlink": "http://stspg.io/803310a12",
  "status": "scheduled",
  "updated_at": "2021-07-02T16:35:05Z"
}
